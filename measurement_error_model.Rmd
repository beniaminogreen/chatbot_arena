---
output : pdf_document
---

# Measurement-Error Models

In this section, we consider two measurement-error models, which aim to provide
accurate predictions for all four outcomes while respecting the ordered nature
of the outcome for games that do not end in a 'both-bad' result.

In these models, we assume that the "A win", "B win", and "tie" outcomes can be
well-described by an ordered logistic regression model. This kind of model
performs well when predicting the frequency of these three outcomes among games
that did not end in a 'both bad' outcome.

$$
Y_i  ~ \text{ordered_logistic}(theta)
$$

However, across both models, we assume that each model $i$ has an indepent
chance of a 'faliure', which can interfere with our observation. Specifically,
for each game $k$, each model flips a baised coin with probaility of heads $\rho_i$
to get an indicator variable $X_(i,k)$, which determines how their outcome is
observed.

In model 1, we assume that a player failing means they are inelligble to win.
Thus, if neither model fails, then the observed outcome is $Y_i$. If either
fails and the other does not, then their opponent wins. Finally, if both models
fail, then the 'both-bad' outcome is observed. This logic is summarized below:

$$
Y_\text{observed} = 
\begin{cases}
Y_i \quad \quad  (X_i != 1) \& (X_i' != 1) \\
\text{A wins} \quad  \quad (X_i != 1) \& (X_i' = 1) \\
\text{B wins} \quad  \quad (X_i = 1) \& (X_i' != 1) \\
\text{both bad} \quad  \quad (X_i = 1) \& (X_i' = 1)\\
\end{cases}
$$

In model 2, we keep the failure probabilities, but assume that a failure from
either model simply derails the competition and forces a 'both-bad' outcome.

$$
Y_\text{observed} = 
\begin{cases}
Y_i \quad \quad  (X_i != 1) \& (X_i' != 1) \\
\text{both bad} \quad  \quad \text{otherwise}
\end{cases}
$$

The first model is theoretically-plausible, but it does not describe the data
well. Specifically, the MAP solution drastically overstates the probability of
a 'both-bad'outcome, and the predictions perform worse (as measured by
cross-entropy loss) than a naive baseline prediction that assumes every model
has the same chance of winning, tying, loosing or the both-bad outcome. Because
of this, we do not proceed further with this model.

The second model lines up worse with our expectations of how the prompt
responses are scored, but it has excellent preditive performance. This model
performs better than an ordinal-logistic regression model that assumes a
constant global probability of faliure common to all units, and provides
well-calibrated predictions for the match results. 


